DS-PI-Data-Engineering-Henry

Proyecto presentado para la finalización de la carrera Data Science en Henry, orientado a Data Engineering. Propone demostrar el uso de varias herramientas y habilidades.

Entre las habilidades se encuentran:

-Análisis de Datos, Transformación de Datos, ETL, Lógica. -Entre las Herramientas se encuentran:
-Jupyter Notebooks, SQL, Python. -Uso de Librerias de Python: Pandas.


Propuesta:

En este primer proyecto proponemos realizar un proceso de ETL (extract, transform and load) a partir de un conjunto de datos sobre distintas plataformas digitales (Netflix, Disney Plus, Hulu y Amazón)

El proyecto consiste en realizar una ingesta de datos desde diversas fuentes, posteriormente aplicar las transformaciones que consideren pertinentes, y luego disponibilizar los datos limpios para su consulta a través de una API. Esta API deberán construirla en un entorno virtual dockerizado.

Los datos serán provistos en archivos de diferentes extensiones, como *csv* o *json*. Se espera que realicen un EDA para cada dataset y corrijan los tipos de datos, valores nulos y duplicados, entre otras tareas. Posteriormente, tendrán que relacionar los datasets así pueden acceder a su información por medio de consultas a la API.

Las consultas a realizar son:

+ Máxima duración según tipo de film (película/serie), por plataforma y por año:
    El request debe ser: get_max_duration(año, plataforma, [min o season])

+ Cantidad de películas y series (separado) por plataforma
    El request debe ser: get_count_plataform(plataforma)  
  
+ Cantidad de veces que se repite un género y plataforma con mayor frecuencia del mismo.
    El request debe ser: get_listedin('genero')  
    Como ejemplo de género pueden usar 'comedy', el cuál deberia devolverles un cunt de 2099 para la plataforma de Amazon.

+ Actor que más se repite según plataforma y año.
  El request debe ser: get_actor(plataforma, año)

Pasos del proyecto

1. Ingesta y normalización de datos

2. Relacionar el conjunto de datos y crear la tabla necesaria para realizar consultas. Aquí se recomienda corroborar qué datos necesitarán en base a las consultas a realizar y concatenar las 4 tablas

3. Leer documentación en links provistos e indagar sobre Uvicorn, FastAPI y Docker

4. Crear la API en un entorno Docker → leer documentación en links provistos

5. Realizar consultas solicitadas

6. Realizar un video demostrativo